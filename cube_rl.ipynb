{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colors: Number of colors\n",
    "\n",
    "C: number of classes\n",
    "\n",
    "A: actions {l:left, r:right, a:above, b:below}\n",
    "\n",
    "cube: adjacency matrix of the cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the cube adjancy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cube = np.array([[1, 3, 4, 5], [2,0,4,5], [3,1,4,5], [0,2,4,5], [1,3,2,0], [3,1,2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* generate a dataset\n",
    "* Assume that we are generating 10 classes, with 10 colors in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "gamma = 0.9\n",
    "num_colors = 10\n",
    "num_classes = 10\n",
    "num_faces = 6\n",
    "SMALL_NUM = 1e-10\n",
    "initLr = 5e-3\n",
    "lrDecayRate = .995\n",
    "lrDecayFreq = 500\n",
    "momentumValue = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randint(batch_size, size=(batch_size, num_faces))\n",
    "y = np.arange(batch_size)\n",
    "# converting in one-hot representation\n",
    "X_train = np.zeros((batch_size, num_faces, num_colors))\n",
    "for i in range(batch_size):\n",
    "    for j in range(num_faces):\n",
    "        X_train[i, j, X[i, j]] = 1\n",
    "\n",
    "# Make the y one-hot representation\n",
    "y_train = np.zeros((batch_size, num_classes))\n",
    "for i in range(batch_size):\n",
    "    y_train[i, y[i]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel(kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "                             initializer=tf.random_normal_initializer())\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_batch(x, init_view, actions, cube):\n",
    "    \"\"\" Get the slice of the next input from the placeholder x\n",
    "    given the next location (left, right, above, below) to look at.\n",
    "    One thing to make sure that you need to first sample to face index\n",
    "    from the cube adjacency matrix, given the current face and the action\n",
    "    taken. \n",
    "    : param x: complete data matrix, of shape: (batch_size, num_faces, num_colors)\n",
    "    : param init_view: the current faces in the batch_x\n",
    "    : param actions: the action taken (l, r, a, b)\n",
    "    : param cube: the cube data adjacency matrix.\n",
    "    : return batch_x: the next_batch \n",
    "    \"\"\"\n",
    "    # from actions, find the face index\n",
    "#     face_index = []\n",
    "    size = [1, num_colors]\n",
    "    batch_x = []\n",
    "    for i in range(batch_size):\n",
    "        face_index = cube[init_view[i, 0], actions[i, 0]]\n",
    "        begin = [face_index, 0]\n",
    "        z = tf.slice(x[i, :, :], begin=begin, size=size)\n",
    "        z = tf.reshape(z, shape=[tf.shape(z)[-1]])\n",
    "        batch_x.append(z)\n",
    "    batch_x = tf.pack(next_batch, axis=0)\n",
    "    return batch_x\n",
    "\n",
    "def get_init_view(x, init_view):\n",
    "    \"\"\"Given the complete data matrix, give me the batch_x\n",
    "    :param x: complete data matrix, of shape: (batch_size, num_faces, num_colors)\n",
    "    :param init_view: the indices of the faces of the cube that \n",
    "    you see on the first time\n",
    "    :return batch_x: the batch for learning of shape (batch_size x num_colors)\n",
    "    the colors are represented in one-hot way.\n",
    "    \"\"\"\n",
    "    init_view = tf.argmax(init_view, )\n",
    "    size = [1, num_colors]\n",
    "    batch_x = []\n",
    "    for i in range(batch_size):\n",
    "        begin = [init_view[i, 0], 0]\n",
    "        z = tf.slice(x[i, :, :], begin=begin, size=size)\n",
    "        z = tf.reshape(z, shape=[tf.shape(z)[-1]])\n",
    "        batch_x.append(z)\n",
    "    batch_x = tf.pack(next_batch, axis=0)\n",
    "    return batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    x = tf.placeholder(name=\"input_data\", shape=[None, num_faces, num_colors], dtype=tf.float32) # total number of colors are 10 in one hot way\n",
    "    y = tf.placeholder(name=\"labels\", shape=[None, num_classes], dtype=tf.float32)\n",
    "    cube = tf.constant(cube)\n",
    "    COSTS = []\n",
    "    Rewards = []\n",
    "    outputs = []\n",
    "    REUSE = False\n",
    "    samp_actions = []\n",
    "    ys = []\n",
    "\n",
    "    # randomly generate first input view location.\n",
    "    init_view = tf.random_uniform((batch_size, 1), minval=0, maxval=5.99)\n",
    "    init_view = tf.to_int32(init_view, name='ToInt32')\n",
    "    batch_x = get_init_view(x, init_view)\n",
    "    for i in range(num_faces):\n",
    "        with tf.variable_scope(\"hidden\", reuse=REUSE):\n",
    "            W, b = get_kernel(kernel_shape=[10, 30], bias_shape=[30])\n",
    "\n",
    "        with tf.variable_scope(\"class\", reuse=REUSE):\n",
    "            W_c, b_c = get_kernel(kernel_shape=[30, 10], bias_shape=[10])\n",
    "\n",
    "        with tf.variable_scope(\"action\", reuse=REUSE):\n",
    "            W_a, b_a = get_kernel(kernel_shape=[30, 4], bias_shape=[4])\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"hidden_proj\") as scope:\n",
    "            # Construct a linear model\n",
    "            hidden = tf.matmul(batch_x, W) + b\n",
    "        \n",
    "        with tf.name_scope(\"classification\") as scope:\n",
    "            probs = tf.nn.softmax(tf.matmul(hidden, W_c) + b_c)  # Softmax prob for classes\n",
    "        \n",
    "        mean_actions = tf.nn.softmax(tf.matmul(hidden, W_a) + b_a)  # Softmax prob for actions\n",
    "        tf.stop_gradient(mean_actions)\n",
    "        \n",
    "        # sample from these actions\n",
    "        samp_action = tf.multinomial(mean_actions, 1,)\n",
    "        samp_action = tf.cast(samp_action, dtype=tf.int32)\n",
    "        tf.stop_gradient(samp_action)\n",
    "        \n",
    "        # define the rewards\n",
    "        max_p_y = tf.arg_max(probs, 1)\n",
    "        max_y = tf.arg_max(y, 1)\n",
    "\n",
    "        equals = tf.equal(max_p_y,max_y)\n",
    "        equals = tf.cast(equals, tf.float32)\n",
    "        acc = tf.reduce_mean(equals)\n",
    "        \n",
    "        R = equals\n",
    "        R = tf.reshape(R, (batch_size, 1))\n",
    "        \n",
    "        # Loss function corresponding to the policy\n",
    "        # I guess this is the probility of the action getting selected under softmax assumption\n",
    "        with tf.name_scope(\"policy_gradient\") as scope:\n",
    "#             p_loc = tf.zeros_like(samp_action, dtype=tf.float32)\n",
    "            p_loc = []\n",
    "            for index in range(batch_size):\n",
    "                p_loc.append(mean_actions[i, samp_action[i, 0]])\n",
    "            p_loc = tf.pack(p_loc, axis=0)\n",
    "            loss_p = tf.log(p_loc) * (R) #*gamma**i)\n",
    "        # Loss function correponding to the class  lychee fruit.prediction\n",
    "        with tf.name_scope(\"class_loss\") as scope:\n",
    "            # Minimize error using cross entropy, y is in one-hot representation\n",
    "            loss_class = y * tf.log(probs)\n",
    "            \n",
    "        J = tf.concat(1, [loss_p, loss_class])\n",
    "        J = tf.reduce_sum(J, 1)\n",
    "        J = tf.reduce_mean(J, 0)\n",
    "\n",
    "        cost = -J\n",
    "        COSTS.append(cost)\n",
    "        samp_actions.append(samp_action)\n",
    "        accuracies.append(acc)\n",
    "        Rewards.append(R)\n",
    "#         outputs.append()\n",
    "        \n",
    "        # generate new views\n",
    "        batch_x = get_next_batch(x, batch_x, samp_action, cube)\n",
    "        \n",
    "        # Don't use the initialized variable again\n",
    "        REUSE = True\n",
    "        \n",
    "    COSTS = tf.reduce_sum(COSTS)\n",
    "    reward = tf.reduce_mean(Rewards)\n",
    "    accuracy = tf.reduce_sum(accuracies)\n",
    "    with tf.name_scope(\"train\") as scope:\n",
    "        # Gradient descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(lr).minimize(COSTS)\n",
    "    return x, y, COSTS, reward, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lr = tf.train.exponential_decay(initLr, global_step, lrDecayFreq, lrDecayRate, staircase=True)\n",
    "x, y, costs, reward, acc = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     v = tf.constant(value=np.zeros((10, 6, 10)))\n",
    "# #     zero = tf.constant(np.arange(2))\n",
    "# #     first = tf.constant(np.arange(10))\n",
    "# #     last = tf.constant(np.arange(6))\n",
    "# #     z = v[first, zero, last]\n",
    "# #     indices = tf.constant(5)\n",
    "# # #     zero = tf.constant(0)\n",
    "# #     begin = tf.pack([indices, 0])\n",
    "# #     shape_ = tf.constant([1, 10])\n",
    "# #     zs = []\n",
    "# #     for i in range(10):\n",
    "# #         z = tf.slice(v[i, 1, :], begin=[indices, 0], size=[1, 10])\n",
    "# #         shape_z = tf.shape(z)\n",
    "# #         z = tf.reshape(z, shape=[shape_z[-1]])\n",
    "# #         zs.append(z)\n",
    "# #     tf.global_variables_initializer()\n",
    "# #     zs = tf.pack(zs, axis=0)\n",
    "#     l = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])\n",
    "#     zs = v[0, :, l]\n",
    "#     print (sess.run(zs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
